{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TwistedGrims/sdfsdf32423dsfsdzxvczxfweqr2314/blob/main/wan22_video_generation_google_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Setup Environment\n",
        "!pip install torch==2.6.0 torchvision==0.21.0\n",
        "%cd /content\n",
        "from IPython.display import clear_output\n",
        "!pip install -q torchsde einops diffusers accelerate xformers==0.0.29.post2 triton==3.2.0 sageattention\n",
        "!pip install av spandrel albumentations insightface onnx opencv-python segment_anything ultralytics onnxruntime\n",
        "!pip install onnxruntime-gpu -y\n",
        "!git clone --branch ComfyUI_v0.3.47 https://github.com/Isi-dev/ComfyUI\n",
        "%cd /content/ComfyUI/custom_nodes\n",
        "!git clone https://github.com/Isi-dev/ComfyUI_GGUF.git\n",
        "!git clone --branch kjnv1.1.3 https://github.com/Isi-dev/ComfyUI_KJNodes.git\n",
        "%cd /content/ComfyUI/custom_nodes/ComfyUI_GGUF\n",
        "!pip install -r requirements.txt\n",
        "%cd /content/ComfyUI/custom_nodes/ComfyUI_KJNodes\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/Isi-dev/Practical-RIFE\n",
        "%cd /content/Practical-RIFE\n",
        "!pip install git+https://github.com/rk-exxec/scikit-video.git@numpy_deprecation\n",
        "!mkdir -p /content/Practical-RIFE/train_log\n",
        "!wget -q https://huggingface.co/Isi99999/Frame_Interpolation_Models/resolve/main/4.25/train_log/IFNet_HDv3.py -O /content/Practical-RIFE/train_log/IFNet_HDv3.py\n",
        "!wget -q https://huggingface.co/Isi99999/Frame_Interpolation_Models/resolve/main/4.25/train_log/RIFE_HDv3.py -O /content/Practical-RIFE/train_log/RIFE_HDv3.py\n",
        "!wget -q https://huggingface.co/Isi99999/Frame_Interpolation_Models/resolve/main/4.25/train_log/refine.py -O /content/Practical-RIFE/train_log/refine.py\n",
        "!wget -q https://huggingface.co/Isi99999/Frame_Interpolation_Models/resolve/main/4.25/train_log/flownet.pkl -O /content/Practical-RIFE/train_log/flownet.pkl\n",
        "%cd /content/ComfyUI\n",
        "!apt -y install -qq aria2 ffmpeg\n",
        "clear_output()\n",
        "print(\"✅ Environment Setup Complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ys6-TUIGoVZJ",
        "outputId": "31cd7c3e-147f-45fe-8b48-dc9b096f3a7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Environment Setup Complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "civitai_token = \"4b00b6113827e77bff1b052404c2f5be\"\n",
        "\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import gc\n",
        "import sys\n",
        "import random\n",
        "import imageio\n",
        "import subprocess\n",
        "import shutil\n",
        "from google.colab import files\n",
        "from IPython.display import display, HTML, Image as IPImage\n",
        "sys.path.insert(0, '/content/ComfyUI')\n",
        "\n",
        "from comfy import model_management\n",
        "from nodes import (\n",
        "    CheckpointLoaderSimple,\n",
        "    CLIPLoader,\n",
        "    CLIPTextEncode,\n",
        "    VAEDecode,\n",
        "    VAELoader,\n",
        "    KSampler,\n",
        "    KSamplerAdvanced,\n",
        "    UNETLoader,\n",
        "    LoadImage,\n",
        "    SaveImage,\n",
        "    CLIPVisionLoader,\n",
        "    CLIPVisionEncode,\n",
        "    LoraLoaderModelOnly,\n",
        "    ImageScale\n",
        ")\n",
        "\n",
        "\n",
        "from custom_nodes.ComfyUI_GGUF.nodes import UnetLoaderGGUF\n",
        "from custom_nodes.ComfyUI_KJNodes.nodes.model_optimization_nodes import (\n",
        "    WanVideoTeaCacheKJ,\n",
        "    PathchSageAttentionKJ,\n",
        "    WanVideoNAG,\n",
        "    SkipLayerGuidanceWanVideo\n",
        ")\n",
        "\n",
        "from comfy_extras.nodes_model_advanced import ModelSamplingSD3\n",
        "from comfy_extras.nodes_images import SaveAnimatedWEBP\n",
        "from comfy_extras.nodes_video import SaveWEBM\n",
        "from comfy_extras.nodes_wan import WanImageToVideo\n",
        "from comfy_extras.nodes_upscale_model import UpscaleModelLoader\n",
        "from comfy_extras.nodes_hunyuan import EmptyHunyuanLatentVideo\n",
        "\n",
        "\n",
        "def download_with_aria2c(link, folder=\"/content/ComfyUI/models/loras\"):\n",
        "    import os\n",
        "    filename = link.split(\"/\")[-1]\n",
        "    command = f\"aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {link} -d {folder} -o {filename}\"\n",
        "    print(\"Executing download command:\")\n",
        "    print(command)\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "    get_ipython().system(command)\n",
        "    return filename\n",
        "\n",
        "\n",
        "def download_civitai_model(civitai_link, token, folder=\"/content/ComfyUI/models/loras\"):\n",
        "    import os\n",
        "    import time\n",
        "\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        model_id = civitai_link.split(\"/models/\")[1].split(\"?\")[0]\n",
        "    except IndexError:\n",
        "        raise ValueError(\"Invalid Civitai URL format. Please use a link like: https://civitai.com/api/download/models/1523247?...\")\n",
        "\n",
        "    civitai_url = f\"https://civitai.com/api/download/models/{model_id}?type=Model&format=SafeTensor\"\n",
        "    if token:\n",
        "        civitai_url += f\"&token={token}\"\n",
        "\n",
        "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"model_{timestamp}.safetensors\"\n",
        "\n",
        "    full_path = os.path.join(folder, filename)\n",
        "\n",
        "    download_command = f\"wget --max-redirect=10 --show-progress \\\"{civitai_url}\\\" -O \\\"{full_path}\\\"\"\n",
        "    print(\"Downloading from Civitai...\")\n",
        "\n",
        "    os.system(download_command)\n",
        "\n",
        "    local_path = os.path.join(folder, filename)\n",
        "    if os.path.exists(local_path) and os.path.getsize(local_path) > 0:\n",
        "        print(f\"LoRA downloaded successfully: {local_path}\")\n",
        "        return filename\n",
        "    else:\n",
        "        print(f\"❌ LoRA download failed or file is empty: {local_path}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def download_lora(link, token, folder=\"/content/ComfyUI/models/loras\"):\n",
        "    if \"civitai.com\" in link.lower():\n",
        "        if not token:\n",
        "            raise ValueError(\"Civitai token is required for Civitai downloads\")\n",
        "        return download_civitai_model(link, token, folder)\n",
        "    else:\n",
        "        return download_with_aria2c(link, folder)\n",
        "\n",
        "def model_download(url: str, dest_dir: str, filename: str = None, silent: bool = True) -> bool:\n",
        "    try:\n",
        "        Path(dest_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        if filename is None:\n",
        "            filename = url.split('/')[-1].split('?')[0]\n",
        "\n",
        "        cmd = [\n",
        "            'aria2c',\n",
        "            '--console-log-level=error',\n",
        "            '-c', '-x', '16', '-s', '16', '-k', '1M',\n",
        "            '-d', dest_dir,\n",
        "            '-o', filename,\n",
        "            url\n",
        "        ]\n",
        "        if silent:\n",
        "            cmd.extend(['--summary-interval=0', '--quiet'])\n",
        "            print(f\"Downloading {filename}...\", end=' ', flush=True)\n",
        "\n",
        "        result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
        "\n",
        "        if silent:\n",
        "            print(\"Done!\")\n",
        "        else:\n",
        "            print(f\"Downloaded {filename} to {dest_dir}\")\n",
        "        return filename\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        error = e.stderr.strip() or \"Unknown error\"\n",
        "        print(f\"\\nError downloading {filename}: {error}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "high_noise_gguf_model = \"https://huggingface.co/bullerwins/Wan2.2-T2V-A14B-GGUF/resolve/main/wan2.2_t2v_high_noise_14B_Q5_K_M.gguf\"# @param {\"type\":\"string\"}\n",
        "low_noise_gguf_model = \"https://huggingface.co/bullerwins/Wan2.2-T2V-A14B-GGUF/resolve/main/wan2.2_t2v_low_noise_14B_Q5_K_M.gguf\"# @param {\"type\":\"string\"}\n",
        "lightx2v_rank = \"32\"\n",
        "lora_urls = [\"https://huggingface.co/Instara/instagirl-wan-2.2/resolve/main/Instagirlv2.5-LOW.safetensors\", \"https://huggingface.co/Instara/instagirl-wan-2.2/resolve/main/Instagirlv2.5-HIGH.safetensors\", \"https://civitai.com/api/download/models/2171005?type=Model&format=SafeTensor\"] # @param {\"type\":\"raw\"}\n",
        "lora_urls = [url for url in lora_urls if url]\n",
        "downloaded_loras = []\n",
        "for url in lora_urls:\n",
        "    if url:\n",
        "        filename = download_lora(url, civitai_token)\n",
        "        if filename:\n",
        "            downloaded_loras.append(filename)\n",
        "\n",
        "dit_model = model_download(high_noise_gguf_model, \"/content/ComfyUI/models/diffusion_models\")\n",
        "dit_model2 = model_download(low_noise_gguf_model, \"/content/ComfyUI/models/diffusion_models\")\n",
        "\n",
        "clear_output()\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors -d /content/ComfyUI/models/text_encoders -o umt5_xxl_fp8_e4m3fn_scaled.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors -d /content/ComfyUI/models/vae -o wan_2.1_vae.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors -d /content/ComfyUI/models/clip_vision -o clip_vision_h.safetensors\n",
        "clear_output()\n",
        "\n",
        "lightx2v_lora = model_download(\"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/lightx2v_I2V_14B_480p_cfg_step_distill_rank32_bf16.safetensors\", \"/content/ComfyUI/models/loras\")\n",
        "\n",
        "def upload_file():\n",
        "    os.makedirs('/content/ComfyUI/input', exist_ok=True)\n",
        "    uploaded = files.upload()\n",
        "    paths = []\n",
        "    for filename in uploaded.keys():\n",
        "        src_path = f'/content/ComfyUI/{filename}'\n",
        "        dest_path = f'/content/ComfyUI/input/{filename}'\n",
        "        shutil.move(src_path, dest_path)\n",
        "        paths.append(dest_path)\n",
        "        print(f\"File saved to: {dest_path}\")\n",
        "    return paths[0] if paths else None\n",
        "def upload_fileInt():\n",
        "    os.makedirs('/content/ComfyUI/output', exist_ok=True)\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    paths = []\n",
        "    for filename in uploaded.keys():\n",
        "        src_path = f'/content/ComfyUI/{filename}'\n",
        "        dest_path = f'/content/ComfyUI/output/{filename}'\n",
        "        shutil.move(src_path, dest_path)\n",
        "        paths.append(dest_path)\n",
        "        print(f\"File saved to: {dest_path}\")\n",
        "\n",
        "    return paths[0] if paths else None\n",
        "def extract_frames(video_path, max_frames=None):\n",
        "    vidcap = cv2.VideoCapture(video_path)\n",
        "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "    frames = []\n",
        "\n",
        "    while True:\n",
        "        success, frame = vidcap.read()\n",
        "        if not success or (max_frames and len(frames) >= max_frames):\n",
        "            break\n",
        "\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frame = torch.from_numpy(frame).float() / 255.0\n",
        "        frames.append(frame)\n",
        "\n",
        "    if not frames:\n",
        "        return None, fps\n",
        "\n",
        "    # Stack frames into a batch tensor: (N, H, W, 3)\n",
        "    batch = torch.stack(frames, dim=0)\n",
        "    # print(f\"Extracted {len(frames)} frames (shape: {batch.shape})\")\n",
        "    return batch, fps\n",
        "def select_every_n_frame_tensor(\n",
        "    frames_tensor: torch.Tensor,\n",
        "    fps: float,\n",
        "    n: int,\n",
        "    skip_first: int = 0,\n",
        "    max_output_frames: int = 0\n",
        "):\n",
        "    if frames_tensor is None or frames_tensor.ndim != 4:\n",
        "        raise ValueError(\"frames_tensor must be a 4D tensor of shape (N, H, W, C)\")\n",
        "    if n < 1:\n",
        "        raise ValueError(\"n must be >= 1\")\n",
        "\n",
        "    total_frames = frames_tensor.shape[0]\n",
        "\n",
        "    if skip_first >= total_frames:\n",
        "        print(\"No frames available after skipping.\")\n",
        "        return None, 0.0\n",
        "\n",
        "    frames_to_use = frames_tensor[skip_first:]\n",
        "\n",
        "    # Select every nth frame\n",
        "    selected_frames = frames_to_use[::n]\n",
        "\n",
        "    # Cap output if needed\n",
        "    if max_output_frames > 0 and selected_frames.shape[0] > max_output_frames:\n",
        "        selected_frames = selected_frames[:max_output_frames]\n",
        "\n",
        "    adjusted_fps = fps / n\n",
        "\n",
        "    if max_output_frames:\n",
        "        print(f\"Frame cap: {max_output_frames} -> Final output: {selected_frames.shape[0]} frames\")\n",
        "\n",
        "    return selected_frames, adjusted_fps\n",
        "def swapT(pa, f, s):\n",
        "    if pa == f:\n",
        "        pa = s\n",
        "    return pa\n",
        "\n",
        "\n",
        "\n",
        "def image_width_height(image):\n",
        "    if image.ndim == 4:\n",
        "        _, height, width, _ = image.shape\n",
        "    elif image.ndim == 3:\n",
        "        height, width, _ = image.shape\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported image shape: {image.shape}\")\n",
        "    return width, height\n",
        "\n",
        "def clear_memory():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "    for obj in list(globals().values()):\n",
        "        if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n",
        "            del obj\n",
        "    gc.collect()\n",
        "\n",
        "def save_as_mp4(images, filename_prefix, fps, output_dir=\"/content/ComfyUI/output\"):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_path = f\"{output_dir}/{filename_prefix}.mp4\"\n",
        "\n",
        "    frames = [(img.cpu().numpy() * 255).astype(np.uint8) for img in images]\n",
        "\n",
        "    with imageio.get_writer(output_path, fps=fps) as writer:\n",
        "        for frame in frames:\n",
        "            writer.append_data(frame)\n",
        "\n",
        "    return output_path\n",
        "\n",
        "def save_as_mp4U(images, filename_prefix, fps, output_dir=\"/content/ComfyUI/output\"):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_path = f\"{output_dir}/{filename_prefix}.mp4\"\n",
        "\n",
        "    frames = []\n",
        "    for i, img in enumerate(images):\n",
        "        try:\n",
        "\n",
        "            if isinstance(img, torch.Tensor):\n",
        "                img = img.cpu().numpy()\n",
        "\n",
        "            # print(f\"Frame {i} initial shape: {img.shape}, dtype: {img.dtype}, max: {img.max()}\")  # Debug\n",
        "\n",
        "\n",
        "            if img.max() <= 1.0:\n",
        "                img = (img * 255).astype(np.uint8)\n",
        "            else:\n",
        "                img = img.astype(np.uint8)\n",
        "\n",
        "\n",
        "            if len(img.shape) == 4:  # Batch dimension? (N, C, H, W)\n",
        "                img = img[0]  # Take first image in batch\n",
        "\n",
        "            if len(img.shape) == 3:\n",
        "                if img.shape[0] in (1, 3, 4):  # CHW format\n",
        "                    img = np.transpose(img, (1, 2, 0))\n",
        "                elif img.shape[2] > 4:  # Too many channels\n",
        "                    img = img[:, :, :3]\n",
        "            elif len(img.shape) == 2:\n",
        "                img = np.expand_dims(img, axis=-1)\n",
        "\n",
        "            # print(f\"Frame {i} processed shape: {img.shape}\")  # Debug\n",
        "\n",
        "            # Final validation\n",
        "            if len(img.shape) != 3 or img.shape[2] not in (1, 3, 4):\n",
        "                raise ValueError(f\"Invalid frame shape after processing: {img.shape}\")\n",
        "\n",
        "            frames.append(img)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing frame {i}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    try:\n",
        "        with imageio.get_writer(output_path, fps=fps) as writer:\n",
        "            for i, frame in enumerate(frames):\n",
        "                # print(f\"Writing frame {i} with shape: {frame.shape}\")  # Debug\n",
        "                writer.append_data(frame)\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing video: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    return output_path\n",
        "\n",
        "def save_as_webp(images, filename_prefix, fps, quality=90, lossless=False, method=4, output_dir=\"/content/ComfyUI/output\"):\n",
        "    \"\"\"Save images as animated WEBP using imageio.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_path = f\"{output_dir}/{filename_prefix}.webp\"\n",
        "\n",
        "\n",
        "    frames = [(img.cpu().numpy() * 255).astype(np.uint8) for img in images]\n",
        "\n",
        "\n",
        "    kwargs = {\n",
        "        'fps': int(fps),\n",
        "        'quality': int(quality),\n",
        "        'lossless': bool(lossless),\n",
        "        'method': int(method)\n",
        "    }\n",
        "\n",
        "    with imageio.get_writer(\n",
        "        output_path,\n",
        "        format='WEBP',\n",
        "        mode='I',\n",
        "        **kwargs\n",
        "    ) as writer:\n",
        "        for frame in frames:\n",
        "            writer.append_data(frame)\n",
        "\n",
        "    return output_path\n",
        "\n",
        "def save_as_webm(images, filename_prefix, fps, codec=\"vp9\", quality=32, output_dir=\"/content/ComfyUI/output\"):\n",
        "    \"\"\"Save images as WEBM using imageio.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_path = f\"{output_dir}/{filename_prefix}.webm\"\n",
        "\n",
        "\n",
        "    frames = [(img.cpu().numpy() * 255).astype(np.uint8) for img in images]\n",
        "\n",
        "\n",
        "    kwargs = {\n",
        "        'fps': int(fps),\n",
        "        'quality': int(quality),\n",
        "        'codec': str(codec),\n",
        "        'output_params': ['-crf', str(int(quality))]\n",
        "    }\n",
        "\n",
        "    with imageio.get_writer(\n",
        "        output_path,\n",
        "        format='FFMPEG',\n",
        "        mode='I',\n",
        "        **kwargs\n",
        "    ) as writer:\n",
        "        for frame in frames:\n",
        "            writer.append_data(frame)\n",
        "\n",
        "    return output_path\n",
        "\n",
        "def save_as_image(image, filename_prefix, output_dir=\"/content/ComfyUI/output\"):\n",
        "    \"\"\"Save single frame as PNG image.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_path = f\"{output_dir}/{filename_prefix}.png\"\n",
        "\n",
        "    frame = (image.cpu().numpy() * 255).astype(np.uint8)\n",
        "\n",
        "    Image.fromarray(frame).save(output_path)\n",
        "\n",
        "    return output_path\n",
        "\n",
        "def save_as_image2(image, filename_prefix, output_dir=\"/content/ComfyUI/output\"):\n",
        "    \"\"\"Save single frame as PNG image.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_path = f\"{output_dir}/{filename_prefix}.png\"\n",
        "\n",
        "    if isinstance(image, torch.Tensor):\n",
        "        image = image.cpu().numpy()\n",
        "    if image.ndim == 4:  # Batch dimension\n",
        "        image = image[0]\n",
        "    if image.shape[0] == 3:  # CHW to HWC\n",
        "        image = np.transpose(image, (1, 2, 0))\n",
        "    image = (image * 255).astype(np.uint8)\n",
        "\n",
        "    Image.fromarray(image).save(output_path)\n",
        "    return output_path\n",
        "\n",
        "\n",
        "def upload_image():\n",
        "    from google.colab import files\n",
        "    import os\n",
        "    import shutil\n",
        "\n",
        "    os.makedirs('/content/ComfyUI/input', exist_ok=True)\n",
        "\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Move each uploaded file to ComfyUI input directory\n",
        "    for filename in uploaded.keys():\n",
        "        src_path = f'/content/ComfyUI/{filename}'\n",
        "        dest_path = f'/content/ComfyUI/input/{filename}'\n",
        "\n",
        "        shutil.move(src_path, dest_path)\n",
        "        # print(f\"Image saved to: {dest_path}\")\n",
        "        return dest_path\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "output_path =\"\"\n",
        "output_pathU =\"\"\n",
        "\n",
        "def generate_video(\n",
        "    # image_path: str = None,\n",
        "    lora_strengths: list = None,\n",
        "    rel_l1_thresh: float = 0.275,\n",
        "    start_percent: float = 0.1,\n",
        "    end_percent: float = 1.0,\n",
        "    positive_prompt: str = \"a cute anime girl with massive fennec ears and a big fluffy tail wearing a maid outfit turning around\",\n",
        "    prompt_assist: str = \"walking to viewers\",\n",
        "    negative_prompt: str = \"色调艳丽，过曝，静态，细节模糊不清，字幕，风格，作品，画作，画面，静止，整体发灰，最差质量，低质量，JPEG压缩残留，丑陋的，残缺的，多余的手指，画得不好的手部，画得不好的脸部，畸形的，毁容的，形态畸形的肢体，手指融合，静止不动的画面，杂乱的背景，三条腿，背景人很多，倒着走\",\n",
        "    width: int = 832,\n",
        "    height: int = 480,\n",
        "    seed: int = 82628696717253,\n",
        "    steps: int = 20,\n",
        "    cfg_scale: float = 1.0,\n",
        "    sampler_name: str = \"uni_pc\",\n",
        "    scheduler: str = \"simple\",\n",
        "    frames: int = 33,\n",
        "    fps: int = 16,\n",
        "    output_format: str = \"mp4\",\n",
        "    overwrite: bool = False,\n",
        "    use_lightx2v: bool = False,\n",
        "    lightx2v_Strength: float = 0.80,\n",
        "    use_pusa: bool = False,\n",
        "    pusa_Strength: float = 1.2,\n",
        "    use_sage_attention: bool = True,\n",
        "    enable_flow_shift: bool = True,\n",
        "    shift: float = 8.0,\n",
        "    enable_flow_shift2: bool = True,\n",
        "    shift2: float = 8.0,\n",
        "    end_step1: int = 10,\n",
        "    downloaded_loras: list = None\n",
        "):\n",
        "\n",
        "    with torch.inference_mode():\n",
        "\n",
        "        # Initialize nodes\n",
        "        unet_loader = UnetLoaderGGUF()\n",
        "        pathch_sage_attention = PathchSageAttentionKJ()\n",
        "        # wan_video_nag = WanVideoNAG()\n",
        "        teacache = WanVideoTeaCacheKJ()\n",
        "        model_sampling = ModelSamplingSD3()\n",
        "        clip_loader = CLIPLoader()\n",
        "        clip_encode_positive = CLIPTextEncode()\n",
        "        clip_encode_negative = CLIPTextEncode()\n",
        "        vae_loader = VAELoader()\n",
        "        clip_vision_loader = CLIPVisionLoader()\n",
        "        clip_vision_encode = CLIPVisionEncode()\n",
        "        load_image = LoadImage()\n",
        "        wan_image_to_video = WanImageToVideo()\n",
        "        ksampler = KSamplerAdvanced()\n",
        "        vae_decode = VAEDecode()\n",
        "        save_webp = SaveAnimatedWEBP()\n",
        "        save_webm = SaveWEBM()\n",
        "        pAssLora = LoraLoaderModelOnly()\n",
        "        load_lora_node = LoraLoaderModelOnly()\n",
        "\n",
        "\n",
        "        # load_causvid_lora = LoraLoaderModelOnly()\n",
        "        load_lightx2v_lora = LoraLoaderModelOnly()\n",
        "        load_pusa_lora = LoraLoaderModelOnly()\n",
        "        image_scaler = ImageScale()\n",
        "        empty_latent_video = EmptyHunyuanLatentVideo()\n",
        "\n",
        "        print(\"Loading Text_Encoder...\")\n",
        "        clip = clip_loader.load_clip(\"umt5_xxl_fp8_e4m3fn_scaled.safetensors\", \"wan\", \"default\")[0]\n",
        "\n",
        "        positive = clip_encode_positive.encode(clip, positive_prompt)[0]\n",
        "        negative = clip_encode_negative.encode(clip, negative_prompt)[0]\n",
        "\n",
        "        del clip\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "        empty_latent = empty_latent_video.generate(width, height, frames, 1)[0]\n",
        "\n",
        "        usedSteps = steps\n",
        "\n",
        "        print(\"Loading high noise Model...\")\n",
        "        model = unet_loader.load_unet(dit_model)[0]\n",
        "\n",
        "        # model = wan_video_nag.patch(model, negative, 11.0, 0.25, 2.5)[0]\n",
        "\n",
        "        if enable_flow_shift:\n",
        "            model = model_sampling.patch(model, shift)[0]\n",
        "\n",
        "        if prompt_assist != \"none\":\n",
        "            if prompt_assist == \"walking to viewers\":\n",
        "                print(\"Loading walking to camera LoRA...\")\n",
        "                model = pAssLora.load_lora_model_only(model, walkingToViewersL, 1)[0]\n",
        "            if prompt_assist == \"walking from behind\":\n",
        "                print(\"Loading walking from camera LoRA...\")\n",
        "                model = pAssLora.load_lora_model_only(model, walkingFromBehindL, 1)[0]\n",
        "            if prompt_assist == \"b3ll13-d8nc3r\":\n",
        "                print(\"Loading dancing LoRA...\")\n",
        "                model = pAssLora.load_lora_model_only(model, dancingL, 1)[0]\n",
        "\n",
        "        if downloaded_loras:\n",
        "            for i, lora_filename in enumerate(downloaded_loras):\n",
        "                if lora_strengths and i < len(lora_strengths):\n",
        "                    lora_strength = lora_strengths[i]\n",
        "                else:\n",
        "                    lora_strength = 1.0 # Default strength\n",
        "\n",
        "                print(f\"Loading LoRA: {lora_filename} with strength {lora_strength}...\")\n",
        "                model = load_lora_node.load_lora_model_only(model, lora_filename, lora_strength)[0]\n",
        "\n",
        "\n",
        "        # if use_causvid:\n",
        "        #     print(\"Loading causvid LoRA...\")\n",
        "        #     model = load_causvid_lora.load_lora_model_only(model, causvid_lora, causvid_Strength)[0]\n",
        "        #     usedSteps=causvid_steps\n",
        "\n",
        "        if use_lightx2v:\n",
        "            print(\"Loading lightx2v LoRA...\")\n",
        "            model = load_lightx2v_lora.load_lora_model_only(model, lightx2v_lora, lightx2v_Strength)[0]\n",
        "            usedSteps=steps\n",
        "\n",
        "        # if use_pusa:\n",
        "        #     print(\"Loading pusav1 LoRA...\")\n",
        "        #     model = load_pusa_lora.load_lora_model_only(model, pusa_lora, pusa_Strength)[0]\n",
        "        #     usedSteps=pusa_steps\n",
        "\n",
        "        if use_sage_attention:\n",
        "            model = pathch_sage_attention.patch(model, \"auto\")[0]\n",
        "\n",
        "        if rel_l1_thresh > 0:\n",
        "            print(\"Setting Teacache...\")\n",
        "            model = teacache.patch_teacache(model, rel_l1_thresh, start_percent, end_percent, \"main_device\", \"14B\")[0]\n",
        "\n",
        "        clear_output()\n",
        "\n",
        "        print(\"Generating video with high noise model...\")\n",
        "        sampled = ksampler.sample(\n",
        "            model=model,\n",
        "            add_noise=\"enable\",\n",
        "            noise_seed=seed,\n",
        "            steps=usedSteps,\n",
        "            cfg=cfg_scale,\n",
        "            sampler_name=sampler_name,\n",
        "            scheduler=scheduler,\n",
        "            positive=positive,\n",
        "            negative=negative,\n",
        "            latent_image=empty_latent,\n",
        "            start_at_step=0,\n",
        "            end_at_step=end_step1,\n",
        "            return_with_leftover_noise=\"enable\"\n",
        "        )[0]\n",
        "\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        print(\"Loading low noise Model...\")\n",
        "        model = unet_loader.load_unet(dit_model2)[0]\n",
        "\n",
        "        # model = wan_video_nag.patch(model, negative, 11.0, 0.25, 2.5)[0]\n",
        "\n",
        "        if enable_flow_shift2:\n",
        "            model = model_sampling.patch(model, shift2)[0]\n",
        "\n",
        "        if prompt_assist != \"none\":\n",
        "            if prompt_assist == \"walking to viewers\":\n",
        "                print(\"Loading walking to camera LoRA...\")\n",
        "                model = pAssLora.load_lora_model_only(model, walkingToViewersL, 1)[0]\n",
        "            if prompt_assist == \"walking from behind\":\n",
        "                print(\"Loading walking from camera LoRA...\")\n",
        "                model = pAssLora.load_lora_model_only(model, walkingFromBehindL, 1)[0]\n",
        "            if prompt_assist == \"b3ll13-d8nc3r\":\n",
        "                print(\"Loading dancing LoRA...\")\n",
        "                model = pAssLora.load_lora_model_only(model, dancingL, 1)[0]\n",
        "\n",
        "        if downloaded_loras:\n",
        "            for i, lora_filename in enumerate(downloaded_loras):\n",
        "                if lora_strengths and i < len(lora_strengths):\n",
        "                    lora_strength = lora_strengths[i]\n",
        "                else:\n",
        "                    lora_strength = 1.0 # Default strength\n",
        "\n",
        "                print(f\"Loading LoRA: {lora_filename} with strength {lora_strength}...\")\n",
        "                model = load_lora_node.load_lora_model_only(model, lora_filename, lora_strength)[0]\n",
        "\n",
        "\n",
        "        # if use_causvid:\n",
        "        #     print(\"Loading causvid LoRA...\")\n",
        "        #     model = load_causvid_lora.load_lora_model_only(model, causvid_lora, causvid_Strength)[0]\n",
        "        #     usedSteps=causvid_steps\n",
        "\n",
        "        # if use_lightx2v:\n",
        "        #     print(\"Loading lightx2v LoRA...\")\n",
        "        #     model = load_lightx2v_lora.load_lora_model_only(model, lightx2v_lora, lightx2v_Strength)[0]\n",
        "        #     usedSteps=lightx2v_steps\n",
        "\n",
        "        if use_pusa:\n",
        "            print(\"Loading pusav1 LoRA...\")\n",
        "            model = load_pusa_lora.load_lora_model_only(model, lightx2v_lora, pusa_Strength)[0]\n",
        "            usedSteps=steps\n",
        "\n",
        "        if use_sage_attention:\n",
        "            model = pathch_sage_attention.patch(model, \"auto\")[0]\n",
        "\n",
        "        if rel_l1_thresh > 0:\n",
        "            print(\"Setting Teacache...\")\n",
        "            model = teacache.patch_teacache(model, rel_l1_thresh, start_percent, end_percent, \"main_device\", \"14B\")[0]\n",
        "\n",
        "        clear_output()\n",
        "\n",
        "        print(\"Generating video with low noise model...\")\n",
        "        sampled = ksampler.sample(\n",
        "            model=model,\n",
        "            add_noise=\"disable\",\n",
        "            noise_seed=seed,\n",
        "            steps=usedSteps,\n",
        "            cfg=cfg_scale,\n",
        "            sampler_name=sampler_name,\n",
        "            scheduler=scheduler,\n",
        "            positive=positive,\n",
        "            negative=negative,\n",
        "            latent_image=sampled,\n",
        "            start_at_step=end_step1,\n",
        "            end_at_step=10000,\n",
        "            return_with_leftover_noise=\"disable\"\n",
        "        )[0]\n",
        "\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        print(\"Loading VAE...\")\n",
        "        vae = vae_loader.load_vae(\"wan_2.1_vae.safetensors\")[0]\n",
        "\n",
        "        try:\n",
        "            print(\"Decoding latents...\")\n",
        "            decoded = vae_decode.decode(vae, sampled)[0]\n",
        "\n",
        "            del vae\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            global output_path\n",
        "            import datetime\n",
        "            base_name = \"ComfyUI\"\n",
        "            if not overwrite:\n",
        "                timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "                base_name += f\"_{timestamp}\"\n",
        "            if frames == 1:\n",
        "                print(\"Single frame detected - saving as PNG image...\")\n",
        "                output_path = save_as_image(decoded[0], \"ComfyUI\")\n",
        "                # print(f\"Image saved as PNG: {output_path}\")\n",
        "\n",
        "                display(IPImage(filename=output_path))\n",
        "            else:\n",
        "                if output_format.lower() == \"webm\":\n",
        "                    print(\"Saving as WEBM...\")\n",
        "                    output_path = save_as_webm(\n",
        "                        decoded,\n",
        "                        base_name,\n",
        "                        fps=fps,\n",
        "                        codec=\"vp9\",\n",
        "                        quality=10\n",
        "                    )\n",
        "                elif output_format.lower() == \"mp4\":\n",
        "                    print(\"Saving as MP4...\")\n",
        "                    output_path = save_as_mp4(decoded, base_name, fps)\n",
        "\n",
        "                    # output_path = save_as_mp4(decoded, \"ComfyUI\", fps)\n",
        "                else:\n",
        "                    raise ValueError(f\"Unsupported output format: {output_format}\")\n",
        "\n",
        "                # print(f\"Video saved as {output_format.upper()}: {output_path}\")\n",
        "\n",
        "                display_video(output_path)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during decoding/saving: {str(e)}\")\n",
        "            raise\n",
        "        finally:\n",
        "            clear_memory()\n",
        "def display_video(video_path):\n",
        "    from IPython.display import HTML\n",
        "    from base64 import b64encode\n",
        "\n",
        "    video_data = open(video_path,'rb').read()\n",
        "\n",
        "    # Determine MIME type based on file extension\n",
        "    if video_path.lower().endswith('.mp4'):\n",
        "        mime_type = \"video/mp4\"\n",
        "    elif video_path.lower().endswith('.webm'):\n",
        "        mime_type = \"video/webm\"\n",
        "    elif video_path.lower().endswith('.webp'):\n",
        "        mime_type = \"image/webp\"\n",
        "    else:\n",
        "        mime_type = \"video/mp4\"  # default\n",
        "\n",
        "    data_url = f\"data:{mime_type};base64,\" + b64encode(video_data).decode()\n",
        "\n",
        "    display(HTML(f\"\"\"\n",
        "    <video width=512 controls autoplay loop>\n",
        "        <source src=\"{data_url}\" type=\"{mime_type}\">\n",
        "    </video>\n",
        "    \"\"\"))\n",
        "\n",
        "clear_output()\n",
        "\n",
        "print(\"✅ Wan 2.2 and Loras Installed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "YTfD3OgHpALT",
        "outputId": "36884ecf-679e-4792-ba40-25c1219bc2bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Wan 2.2 and Loras Installed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown # 2. Video Generation\n",
        "import time\n",
        "start_time = time.time()\n",
        "# @markdown ### Video Settings\n",
        "positive_prompt = \"Dexter morgan from Dexter walking down the street of london, tanks driving through the street behind him\" # @param {\"type\":\"string\"}\n",
        "prompt_assist = \"none\" # @param [\"none\",\"walking to camera\", \"walking from camera\", \"swaying\"]\n",
        "prompt_assist = swapT(prompt_assist, \"walking to camera\", \"walking to viewers\")\n",
        "prompt_assist = swapT(prompt_assist, \"walking from camera\", \"walking from behind\")\n",
        "prompt_assist = swapT(prompt_assist, \"swaying\", \"b3ll13-d8nc3r\")\n",
        "positive_prompt = f\"{positive_prompt} {prompt_assist}.\" if prompt_assist != \"none\" else positive_prompt\n",
        "# positive_prompt = f\"{positive_prompt} Turn this image into {prompt_assist} style.\" if prompt_assist != \"none\" else positive_prompt\n",
        "\n",
        "negative_prompt = \"色调艳丽，过曝，静态，细节模糊不清，字幕，风格，作品，画作，画面，静止，整体发灰，最差质量，低质量，JPEG压缩残留，丑陋的，残缺的，多余的手指，画得不好的手部，画得不好的脸部，畸形的，毁容的，形态畸形的肢体，手指融合，静止不动的画面，杂乱的背景，三条腿，背景人很多，倒着走\" # @param {\"type\":\"string\"}\n",
        "width = 720 # @param {\"type\":\"number\"}\n",
        "height = 1080 # @param {\"type\":\"number\"}\n",
        "seed = 51851858123 # @param {\"type\":\"integer\"}\n",
        "high_noise_steps = 2 # @param {\"type\":\"integer\", \"min\":1, \"max\":25}\n",
        "steps = 4 # @param {\"type\":\"integer\", \"min\":1, \"max\":50}\n",
        "cfg_scale = 1.5 # @param {\"type\":\"number\", \"min\":1, \"max\":20}\n",
        "sampler_name = \"euler\" # @param [\"uni_pc\", \"uni_pc_bh2\", \"ddim\",\"euler\", \"euler_cfg_pp\", \"euler_ancestral\", \"euler_ancestral_cfg_pp\", \"heun\", \"heunpp2\",\"dpm_2\", \"dpm_2_ancestral\",\"lms\", \"dpm_fast\", \"dpm_adaptive\", \"dpmpp_2s_ancestral\", \"dpmpp_2s_ancestral_cfg_pp\", \"dpmpp_sde\", \"dpmpp_sde_gpu\",\"dpmpp_2m\", \"dpmpp_2m_cfg_pp\", \"dpmpp_2m_sde\", \"dpmpp_2m_sde_gpu\", \"dpmpp_3m_sde\", \"dpmpp_3m_sde_gpu\", \"ddpm\", \"lcm\",\"ipndm\", \"ipndm_v\", \"deis\", \"res_multistep\", \"res_multistep_cfg_pp\", \"res_multistep_ancestral\", \"res_multistep_ancestral_cfg_pp\",\"gradient_estimation\", \"er_sde\", \"seeds_2\", \"seeds_3\"]\n",
        "scheduler = \"simple\" # @param [\"simple\",\"normal\",\"karras\",\"exponential\",\"sgm_uniform\",\"ddim_uniform\",\"beta\",\"linear_quadratic\",\"kl_optimal\"]\n",
        "frames = 60 # @param {\"type\":\"integer\", \"min\":1, \"max\":120}\n",
        "# fps = 16 # @param {\"type\":\"integer\", \"min\":1, \"max\":60}\n",
        "fps = 16\n",
        "# output_format = \"mp4\" # @param [\"mp4\", \"webm\"]\n",
        "output_format = \"mp4\"\n",
        "overwrite_previous_video = True # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown ### Model Configuration\n",
        "use_sage_attention = True # @param {type:\"boolean\"}\n",
        "# use_sage_attention = True\n",
        "use_flow_shift = True # @param {type:\"boolean\"}\n",
        "flow_shift = 8 # @param {\"type\":\"slider\",\"min\":0.0,\"max\":100.0,\"step\":0.01}\n",
        "flow_shift2 = 8 # @param {\"type\":\"slider\",\"min\":0.0,\"max\":100.0,\"step\":0.01}\n",
        "\n",
        "\n",
        "# @markdown ### Wan2.1 Based Models LoRA Configuration\n",
        "# use_causvid = False # @param {type:\"boolean\"}\n",
        "# causvid_Strength = 0.8 # @param {\"type\":\"slider\",\"min\":-100,\"max\":100,\"step\":0.01}\n",
        "# causvid_steps = 4 # @param {\"type\":\"integer\", \"min\":1, \"max\":20}\n",
        "use_lightx2v = True # @param {type:\"boolean\"}\n",
        "lightx2v_Strength = 3 # @param {\"type\":\"slider\",\"min\":-10,\"max\":10,\"step\":0.01}\n",
        "# lightx2v_steps = 4 # @param {\"type\":\"integer\", \"min\":1, \"max\":20}\n",
        "use_lightx2v2 = True # @param {type:\"boolean\"}\n",
        "lightx2v2_Strength = 1.5 # @param {\"type\":\"slider\",\"min\":-10,\"max\":10,\"step\":0.01}\n",
        "# pusav1_steps = 6 # @param {\"type\":\"integer\", \"min\":1, \"max\":20}\n",
        "\n",
        "\n",
        "# @markdown ### LoRA Configuration\n",
        "# You can add multiple LoRA URLs in the list in the cell above (YTfD3OgHpALT)\n",
        "# The strengths for each LoRA should be provided as a list here, corresponding to the order of URLs in the cell above.\n",
        "lora_strengths = [1.0, 0.62, 1.0] # @param {\"type\":\"raw\"}\n",
        "\n",
        "\n",
        "# @markdown ### Teacache Settings\n",
        "rel_l1_thresh = 0 # @param {\"type\":\"slider\",\"min\":0.0,\"max\":10,\"step\":0.001}\n",
        "start_percent = 0.2 # @param {\"type\":\"slider\",\"min\":0.0,\"max\":1.0,\"step\":0.01}\n",
        "end_percent = 1.0 # @param {\"type\":\"slider\",\"min\":0.0,\"max\":1.0,\"step\":0.01}\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "import random\n",
        "seed = seed if seed != 0 else random.randint(0, 2**32 - 1)\n",
        "print(f\"Using seed: {seed}\")\n",
        "\n",
        "# with torch.inference_mode():\n",
        "generate_video(\n",
        "    lora_strengths=lora_strengths,\n",
        "    rel_l1_thresh=rel_l1_thresh,\n",
        "    start_percent=start_percent,\n",
        "    end_percent = end_percent,\n",
        "    positive_prompt=positive_prompt,\n",
        "    prompt_assist=prompt_assist,\n",
        "    negative_prompt=negative_prompt,\n",
        "    width=width,\n",
        "    height=height,\n",
        "    seed=seed,\n",
        "    steps=steps,\n",
        "    cfg_scale=cfg_scale,\n",
        "    sampler_name=sampler_name,\n",
        "    scheduler=scheduler,\n",
        "    frames=frames,\n",
        "    fps=fps,\n",
        "    output_format=output_format,\n",
        "    overwrite=overwrite_previous_video,\n",
        "    use_lightx2v=use_lightx2v,\n",
        "    lightx2v_Strength=lightx2v_Strength,\n",
        "    use_pusa=use_lightx2v2,\n",
        "    pusa_Strength=lightx2v2_Strength,\n",
        "    use_sage_attention = use_sage_attention,\n",
        "    enable_flow_shift = use_flow_shift,\n",
        "    shift = flow_shift,\n",
        "    enable_flow_shift2 = use_flow_shift,\n",
        "    shift2 = flow_shift2,\n",
        "    end_step1 = high_noise_steps,\n",
        "    downloaded_loras=downloaded_loras # Pass the list of downloaded LoRAs\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "duration = end_time - start_time\n",
        "mins, secs = divmod(duration, 60)\n",
        "print(f\"Seed: {seed}\")\n",
        "# print(f\"prompt: {positive_prompt}\")\n",
        "print(f\"✅ Generation completed in {int(mins)} min {secs:.2f} sec\")\n",
        "\n",
        "clear_memory()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "AqcMR3p7sCpw",
        "outputId": "b5f6fdf8-0074-4515-d242-2d5d520f7974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using seed: 51851858123\n",
            "Loading Text_Encoder...\n"
          ]
        }
      ]
    }
  ]
}